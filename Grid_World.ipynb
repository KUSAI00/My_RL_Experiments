{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Pg2Rdi2_6K6d"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Y1likLylLCAk"
   },
   "outputs": [],
   "source": [
    "n = 5 # dimension of the matrix\n",
    "states = [(r,c) for r in range(n) for c in range(n)] # States: (row, col)\n",
    "\n",
    "# Actions: Up, Down, Left, Right\n",
    "actions = [\"U\",\"D\",\"L\",\"R\"]\n",
    "action_effects = {\"U\": (-1, 0), \"D\":(1,0), \"L\":(0,-1), \"R\": (0,1) }\n",
    "\n",
    "# -10 is hazard, +10 is goal\n",
    "rewards = {(1,0): -10,\n",
    "           (1,2): -10,\n",
    "           (0,2): -10,\n",
    "           (3,1): -10,\n",
    "           (3,3): -10,\n",
    "           (3,4): -10,\n",
    "           (4,4): +10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "db07b127",
    "outputId": "184db3f8-4286-4582-9e71-e1ddde96d28e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards Matrix:\n",
      "   0    0  -10    0    0\n",
      " -10    0  -10    0    0\n",
      "   0    0    0    0    0\n",
      "   0  -10    0  -10  -10\n",
      "   0    0    0    0   10\n",
      "\n",
      "States Matrix:\n",
      "(0, 0) (0, 1) (0, 2) (0, 3) (0, 4)\n",
      "(1, 0) (1, 1) (1, 2) (1, 3) (1, 4)\n",
      "(2, 0) (2, 1) (2, 2) (2, 3) (2, 4)\n",
      "(3, 0) (3, 1) (3, 2) (3, 3) (3, 4)\n",
      "(4, 0) (4, 1) (4, 2) (4, 3) (4, 4)\n"
     ]
    }
   ],
   "source": [
    "#Visualize the rewards of each state\n",
    "\n",
    "# Create a list of rewards corresponding to the states list\n",
    "rewards_list = [rewards.get(state, 0) for state in states]\n",
    "\n",
    "def print_states_as_matrix(states, rows, cols, data_list):\n",
    "    matrix = [[None for _ in range(cols)] for _ in range(rows)] # Initialize with None\n",
    "    for i, (r, c) in enumerate(states):\n",
    "        matrix[r][c] = data_list[i]\n",
    "\n",
    "    # Format and print the matrix for better alignment\n",
    "    for row in matrix:\n",
    "        # Adjust formatting to accommodate tuples (states)\n",
    "        formatted_row = [f\"{str(cell): >4}\" for cell in row] # Use a larger width for states\n",
    "        print(\" \".join(formatted_row))\n",
    "\n",
    "print(\"Rewards Matrix:\")\n",
    "print_states_as_matrix(states, n, n, rewards_list)\n",
    "\n",
    "print(\"\\nStates Matrix:\")\n",
    "print_states_as_matrix(states, n, n, states) # Print states as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "mWBI1Bg0S2St"
   },
   "outputs": [],
   "source": [
    "# Transition function: P(s'|s, a)\n",
    "\n",
    "def transition(state, action):\n",
    "  if state in rewards.keys():  # Terminal states\n",
    "    return state # terminate\n",
    "\n",
    "  # Intende move (80% chance)\n",
    "  dr, dc = action_effects[action]\n",
    "  test_state = (state[0] + dr, state[1] + dc)\n",
    "  if test_state in states:\n",
    "    new_state = test_state\n",
    "  else:\n",
    "    new_state = state\n",
    "\n",
    "  # Slipping sideways (20% chance)\n",
    "  if np.random.random() < 0.2:\n",
    "    slip_action = np.random.choice([a for a in actions if a != action])\n",
    "    dr, dc = action_effects[slip_action]\n",
    "    test_state = (state[0] + dr, state[1] + dc)\n",
    "    if test_state in states:\n",
    "      new_state = test_state\n",
    "    else:\n",
    "      new_state = state\n",
    "\n",
    "  return new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "bV9dR5G9Y-Ln"
   },
   "outputs": [],
   "source": [
    "# Simulate an episode\n",
    "def simulate(policy=None, initial_state=(0,0), max_steps=100):\n",
    "  # Initialize the agent's state and track its path\n",
    "  state = initial_state\n",
    "  path = [state]\n",
    "  total_reward = 0\n",
    "\n",
    "  for _ in range(max_steps):\n",
    "    # Check if current state is terminal\n",
    "    if state in rewards:\n",
    "      total_reward += rewards[state]\n",
    "      break #terminate\n",
    "\n",
    "    # Default random policy if none provided\n",
    "    if policy is None:\n",
    "        action = np.random.choice(actions)\n",
    "    else:\n",
    "        action = policy.get(state, np.random.choice(actions))\n",
    "\n",
    "    # Execute action and transition to next state\n",
    "    state = transition(state, action)\n",
    "    path.append(state)\n",
    "    total_reward -= 1  # Step cost\n",
    "\n",
    "  return path, total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pWbQoGbAbiHh",
    "outputId": "55c1815a-631b-4763-8e7f-60c9a376689b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path: [(0, 0), (0, 0), (0, 1), (1, 1), (0, 1), (1, 1), (1, 0)], Total Reward: -16\n"
     ]
    }
   ],
   "source": [
    "# Random policy\n",
    "path, reward = simulate()\n",
    "print(f\"Path: {path}, Total Reward: {reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wmu7CEL7brLQ",
    "outputId": "d0dd4515-78d1-41fc-8780-4e0b8e42c0a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path: [(0, 0), (0, 0), (0, 1), (1, 1), (1, 2)], Total Reward: -14\n"
     ]
    }
   ],
   "source": [
    "# Custom policy (always go Right then Down)\n",
    "custom_policy = {\n",
    "    (0, 0): \"R\", (0, 1): \"D\", (0, 2): \"D\",\n",
    "    (1, 0): \"R\", (1, 1): \"R\", (1, 2): \"D\",\n",
    "    (2, 0): \"R\", (2, 1): \"R\",\n",
    "}\n",
    "path, reward = simulate(policy=custom_policy)\n",
    "print(f\"Path: {path}, Total Reward: {reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MBR4IBqTsOSe",
    "outputId": "757bca01-3ebd-4012-d228-bc59b0101020"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path: [(0, 0), (0, 0), (0, 0), (0, 1), (1, 1), (1, 0)], Total Reward: -15\n"
     ]
    }
   ],
   "source": [
    "# Another costum policy that i tried to make it as good as possible manually\n",
    "custom_policy = {\n",
    "    (0, 0):\"R\", (0, 1):\"D\", (0, 2):\"R\", (0, 3):\"D\", (0, 4):\"D\",\n",
    "    (1, 0):\"R\", (1, 1):\"D\", (1, 2):\"R\", (1, 3):\"D\", (1, 4):\"D\",\n",
    "    (2, 0):\"D\", (2, 1):\"R\", (2, 2):\"D\", (2, 3):\"L\", (2, 4):\"L\",\n",
    "    (3, 0):\"D\", (3, 1):\"R\", (3, 2):\"D\", (3, 3):\"R\", (3, 4):\"R\",\n",
    "    (4, 0):\"R\", (4, 1):\"R\", (4, 2):\"R\", (4, 3):\"R\", (4, 4):\"R\",\n",
    "}\n",
    "path, reward = simulate(policy=custom_policy)\n",
    "print(f\"Path: {path}, Total Reward: {reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "aJS6Y7I565i-"
   },
   "outputs": [],
   "source": [
    " #--- Value Iteration (to compute optimal policy) ---\n",
    "def get_transitions(state, action):\n",
    "    transitions = []\n",
    "\n",
    "    # if the current state is a terminal state it stays there\n",
    "    if state in rewards:\n",
    "        return [(state, 1.0)]\n",
    "\n",
    "    # Intended move (80%)\n",
    "    dr, dc = action_effects[action]\n",
    "    # The new state is clipped to stay within grid bounds\n",
    "    new_state = (max(0, min(n-1, state[0] + dr))), (max(0, min(n-1, state[1] + dc)))\n",
    "    transitions.append((new_state, 0.8))\n",
    "\n",
    "    # Slip sideways (20%)\n",
    "    slip_actions = [a for a in actions if a != action]\n",
    "    for a in slip_actions:\n",
    "        dr, dc = action_effects[a]\n",
    "        new_state = (max(0, min(n-1, state[0] + dr))), (max(0, min(n-1, state[1] + dc)))\n",
    "        #Each slip action has equal probability (0.2 / (num_actions - 1))\n",
    "        transitions.append((new_state, 0.2 / (len(actions) - 1)))\n",
    "\n",
    "    return transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NjFsNq5BvDjg"
   },
   "outputs": [],
   "source": [
    "def value_iteration(gamma=1, theta=1e-6):\n",
    "    V = {s: 0 for s in states}\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in states:\n",
    "            if s in rewards:\n",
    "                V[s] = rewards[s]\n",
    "                continue\n",
    "            v_old = V[s]\n",
    "            max_value = -float('inf')\n",
    "            for a in actions:\n",
    "                expected_value = 0\n",
    "                for (s_prime, prob) in get_transitions(s, a):\n",
    "                    reward = rewards.get(s_prime, -1)  # Default step cost = -1\n",
    "                    if s_prime in rewards:\n",
    "                        expected_value += prob * reward\n",
    "                    else:\n",
    "                        expected_value += prob * (reward + gamma * V[s_prime])\n",
    "                if expected_value > max_value:\n",
    "                    max_value = expected_value\n",
    "            V[s] = max_value\n",
    "            delta = max(delta, abs(v_old - V[s]))\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    # Extract optimal policy\n",
    "    policy = {}\n",
    "    for s in states:\n",
    "        if s in rewards:\n",
    "            policy[s] = None  # No action in terminal states\n",
    "            continue\n",
    "        best_action = None\n",
    "        best_value = -float('inf')\n",
    "        for a in actions:\n",
    "            expected_value = 0\n",
    "            for (s_prime, prob) in get_transitions(s, a):\n",
    "                reward = rewards.get(s_prime, -1)\n",
    "                expected_value += prob * (reward + gamma * V[s_prime])\n",
    "            if expected_value > best_value:\n",
    "                best_value = expected_value\n",
    "                best_action = a\n",
    "        policy[s] = best_action\n",
    "    return V, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gjz7BjeExInH",
    "outputId": "a4331199-7d02-45c3-ff1d-c425d3bbda9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1:\n",
      "  Path: [(0, 0), (0, 0), (0, 0), (1, 0)]\n",
      "  Total Reward: -13\n",
      "--------------------------------------------------\n",
      "Episode 2:\n",
      "  Path: [(0, 0), (0, 1), (1, 1), (2, 1), (2, 2), (3, 2), (4, 2), (4, 3), (4, 4)]\n",
      "  Total Reward: 2\n",
      "--------------------------------------------------\n",
      "Episode 3:\n",
      "  Path: [(0, 0), (0, 1), (1, 1), (2, 1), (2, 2), (3, 2), (4, 2), (4, 3), (4, 4)]\n",
      "  Total Reward: 2\n",
      "--------------------------------------------------\n",
      "Episode 4:\n",
      "  Path: [(0, 0), (0, 1), (1, 1), (2, 1), (2, 2), (3, 2), (4, 2), (4, 1), (4, 2), (3, 2), (4, 2), (4, 3), (4, 4)]\n",
      "  Total Reward: -2\n",
      "--------------------------------------------------\n",
      "Episode 5:\n",
      "  Path: [(0, 0), (0, 1), (1, 1), (2, 1), (2, 2), (3, 2), (4, 2), (4, 3), (4, 4)]\n",
      "  Total Reward: 2\n",
      "--------------------------------------------------\n",
      "Optimal Policy Grid (Actions) vs. Value Function (V):\n",
      "--------------------------------------------------\n",
      "    R     D   -10     D     D    |     -6.85   -5.42  -10.00   -5.16   -5.86 \n",
      "  -10     D   -10     D     D    |    -10.00   -3.75  -10.00   -3.53   -4.67 \n",
      "    D     R     D     L     L    |     -1.51   -1.48    0.58   -1.60   -3.42 \n",
      "    D   -10     D   -10   -10    |      0.36  -10.00    2.98  -10.00  -10.00 \n",
      "    R     R     R     R    10    |      2.55    3.98    6.43    8.17   10.00 \n"
     ]
    }
   ],
   "source": [
    "# --- Run Simulation with Optimal Policy ---\n",
    "V, optimal_policy = value_iteration()\n",
    "\n",
    "# Simulate 5 episodes and show paths\n",
    "for episode in range(5):\n",
    "    path, reward = simulate(policy=optimal_policy, initial_state=(0, 0))\n",
    "    print(f\"Episode {episode + 1}:\")\n",
    "    print(f\"  Path: {path}\")\n",
    "    print(f\"  Total Reward: {reward}\")\n",
    "    print(f\"{'-'*50}\")\n",
    "\n",
    "# Print optimal policy (for reference)\n",
    "print(\"Optimal Policy Grid (Actions) vs. Value Function (V):\")\n",
    "print(\"-\" * 50)\n",
    "for r in range(n):\n",
    "    # Print Policy (Actions)\n",
    "    for c in range(n):\n",
    "        s = (r, c)\n",
    "        if s in rewards:\n",
    "            print(f\"{rewards[s]:>5}\", end=\" \")\n",
    "        else:\n",
    "            print(f\"{optimal_policy[s]:>5}\", end=\" \")\n",
    "    print(\"   |   \", end=\"\")\n",
    "\n",
    "    # Print Value Function (V)\n",
    "    for c in range(n):\n",
    "        s = (r, c)\n",
    "        print(f\"{V[s]:>7.2f}\", end=\" \")\n",
    "    print()  # New line after each row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IC15FmZCpYaT"
   },
   "source": [
    "################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oNKzuY_Zp5qs"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "# Simulate environment (unknown dynamics)\n",
    "def step(state, action):\n",
    "    if state in rewards:  # Terminal state\n",
    "        return state, rewards[state], True\n",
    "\n",
    "    # Determine actual action (with slip)\n",
    "    if random.random() < 0.8:\n",
    "        action = action\n",
    "    else:\n",
    "        slip_actions = [a for a in actions if a != action]\n",
    "        action = random.choice(slip_actions)\n",
    "\n",
    "    # Simulate transition (agent doesn't know this!)\n",
    "    dr, dc = action_effects[action]\n",
    "    new_state = (max(0, min(n-1, state[0] + dr)), max(0, min(n-1, state[1] + dc)))\n",
    "    reward = rewards.get(new_state, -1)  # Default step cost = -1\n",
    "    done = new_state in rewards\n",
    "    return new_state, reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j0SmXBCbKDRf"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "alpha = 0.1  # Learning rate\n",
    "gamma = 1  # Discount factor\n",
    "epsilon = 0.2  # Exploration rate\n",
    "episodes = 5000  # Training episodes\n",
    "\n",
    "# Initialize Q-table\n",
    "Q = {(s, a): 0 for s in states for a in actions}\n",
    "\n",
    "\n",
    "for episode in range(episodes):\n",
    "  state = random.choice(states)\n",
    "  done = False\n",
    "\n",
    "  while not done:\n",
    "    # ε-greedy action selection\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        action = random.choice(actions)\n",
    "    else:\n",
    "        action = max(actions, key=lambda a: Q[(state, a)])\n",
    "\n",
    "    # Take action, observe next state and reward\n",
    "    next_state, reward, done = step(state, action)\n",
    "\n",
    "    # Update Q-table\n",
    "    best_next_action = max(actions, key=lambda a: Q[(next_state, a)])\n",
    "    if done:\n",
    "        td_target = reward\n",
    "    else:\n",
    "        td_target = reward + gamma * Q[(next_state, best_next_action)]\n",
    "    td_error = td_target - Q[(state, action)]\n",
    "    Q[(state, action)] += alpha * td_error\n",
    "\n",
    "    state = next_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "puEi-BFstebN",
    "outputId": "a3b86fe4-35b8-4eea-eaf2-ef5e55da8e97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Policy (Learned via Q-Learning):\n",
      "    R     D   -10     D     D \n",
      "  -10     D   -10     D     D \n",
      "    D     R     D     L     L \n",
      "    D   -10     D   -10   -10 \n",
      "    R     R     R     R    10 \n",
      "--------------------------------------------------\n",
      "Optimal Policy (Learned via Q-Learning) and Q-values:\n",
      "   -5.6    -4.3     -10    -4.1    -5.2 \n",
      "    -10    -3.7     -10    -1.8    -3.9 \n",
      "   -1.5     0.4     2.2     0.7    -2.0 \n",
      "    1.1     -10     3.8     -10     -10 \n",
      "    3.0     4.3     5.9     7.3      10 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Extract optimal policy from Q-table\n",
    "policy = {}\n",
    "for s in states:\n",
    "    if s in rewards:\n",
    "        policy[s] = None  # Terminal state\n",
    "    else:\n",
    "        policy[s] = max(actions, key=lambda a: Q[(s, a)])\n",
    "\n",
    "# Print policy\n",
    "print(\"Optimal Policy (Learned via Q-Learning):\")\n",
    "for r in range(n):\n",
    "    for c in range(n):\n",
    "        s = (r, c)\n",
    "        if s in rewards:\n",
    "            print(f\"{rewards[s]:>5}\", end=\" \")\n",
    "        else:\n",
    "            print(f\"{policy[s]:>5}\", end=\" \")\n",
    "    print()\n",
    "print(\"-\" *50)\n",
    "print(\"Optimal Policy (Learned via Q-Learning) and Q-values:\")\n",
    "for r in range(n):\n",
    "    for c in range(n):\n",
    "        s = (r, c)\n",
    "        if s in rewards:  # Terminal state (no action)\n",
    "            print(f\"{rewards[s]:>7}\", end=\" \")\n",
    "        else:\n",
    "            best_action = policy[s]\n",
    "            best_q = Q[(s, best_action)]\n",
    "            print(f\"{best_q:>7.1f}\", end=\" \")\n",
    "    print()  # Newline after each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lA9iD8M6tzVL"
   },
   "outputs": [],
   "source": [
    "# Known transition function (used in step())\n",
    "def known_transition(state, action):\n",
    "    if state in rewards:  # Terminal state\n",
    "        return state, rewards[state], True\n",
    "\n",
    "    # Probabilistic transitions (80% intended, 20% slip)\n",
    "    if random.random() < 0.8:\n",
    "        new_state = (max(0, min(n-1, state[0] + action_effects[action][0])),\n",
    "                    max(0, min(n-1, state[1] + action_effects[action][1])))\n",
    "    else:\n",
    "        # Slip sideways (equal probability for other actions)\n",
    "        slip_actions = [a for a in actions if a != action]\n",
    "        slip_action = random.choice(slip_actions)\n",
    "        new_state = (max(0, min(n-1, state[0] + action_effects[slip_action][0])),\n",
    "                    max(0, min(n-1, state[1] + action_effects[slip_action][1])))\n",
    "\n",
    "    reward = rewards.get(new_state, -1)  # Default step cost = -1\n",
    "    done = new_state in rewards\n",
    "    return new_state, reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y9UXSZnp0PzN"
   },
   "outputs": [],
   "source": [
    "# Q-Learning (same as before, but uses known_transition)\n",
    "for episode in range(episodes):\n",
    "    state = random.choice(states)  # Random start state\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        # ε-greedy action selection\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = random.choice(actions)\n",
    "        else:\n",
    "            action = max(actions, key=lambda a: Q[(state, a)])\n",
    "\n",
    "        # Take action using known dynamics\n",
    "        next_state, reward, done = known_transition(state, action)\n",
    "\n",
    "        # Update Q-table\n",
    "        best_next_action = max(actions, key=lambda a: Q[(next_state, a)])\n",
    "        if done:\n",
    "            td_target = reward\n",
    "        else:\n",
    "            td_target = reward + gamma * Q[(next_state, best_next_action)]\n",
    "        td_error = td_target - Q[(state, action)]\n",
    "        Q[(state, action)] += alpha * td_error\n",
    "\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_OM1rsww0ZVF",
    "outputId": "9e119af8-756c-43c7-fd39-88113810a832"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Policy (Learned via Q-Learning with Known Dynamics):\n",
      "    R     D   -10     D     D \n",
      "  -10     D   -10     D     D \n",
      "    D     R     D     L     L \n",
      "    D   -10     D   -10   -10 \n",
      "    R     R     R     R    10 \n",
      "--------------------------------------------------\n",
      "Optimal Policy (Learned via Q-Learning) and Q-values:\n",
      "   -5.8    -4.9     -10    -4.5    -5.8 \n",
      "    -10    -3.5     -10    -3.2    -4.0 \n",
      "   -1.4    -2.4     0.3    -1.6    -3.6 \n",
      "    0.0     -10     2.0     -10     -10 \n",
      "    1.7     4.0     5.7     8.7      10 \n"
     ]
    }
   ],
   "source": [
    "# Extract optimal policy from Q-table\n",
    "policy = {}\n",
    "for s in states:\n",
    "    if s in rewards:\n",
    "        policy[s] = None  # Terminal state\n",
    "    else:\n",
    "        policy[s] = max(actions, key=lambda a: Q[(s, a)])\n",
    "\n",
    "# Print policy\n",
    "print(\"Optimal Policy (Learned via Q-Learning with Known Dynamics):\")\n",
    "for r in range(n):\n",
    "    for c in range(n):\n",
    "        s = (r, c)\n",
    "        if s in rewards:\n",
    "            print(f\"{rewards[s]:>5}\", end=\" \")\n",
    "        else:\n",
    "            print(f\"{policy[s]:>5}\", end=\" \")\n",
    "    print()\n",
    "\n",
    "print(\"-\" *50)\n",
    "print(\"Optimal Policy (Learned via Q-Learning) and Q-values:\")\n",
    "for r in range(n):\n",
    "    for c in range(n):\n",
    "        s = (r, c)\n",
    "        if s in rewards:  # Terminal state (no action)\n",
    "            print(f\"{rewards[s]:>7}\", end=\" \")\n",
    "        else:\n",
    "            best_action = policy[s]\n",
    "            best_q = Q[(s, best_action)]\n",
    "            print(f\"{best_q:>7.1f}\", end=\" \")\n",
    "    print()  # Newline after each row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ExHVmPvMyCl"
   },
   "source": [
    "################# Q-value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "8wXD9z8GQcbk"
   },
   "outputs": [],
   "source": [
    "# Transition function (known dynamics)\n",
    "def get_transitions(state, action):\n",
    "    transitions = []\n",
    "    if state in rewards:  # Terminal state\n",
    "        return [(state, 1.0, rewards[state])]\n",
    "\n",
    "    # Intended move (80%)\n",
    "    dr, dc = action_effects[action]\n",
    "    new_state = (max(0, min(n-1, state[0] + dr)), max(0, min(n-1, state[1] + dc)))\n",
    "    transitions.append((new_state, 0.8, rewards.get(new_state, -1)))\n",
    "\n",
    "    # Slip sideways (20% split between other actions)\n",
    "    slip_actions = [a for a in actions if a != action]\n",
    "    for a in slip_actions:\n",
    "        dr, dc = action_effects[a]\n",
    "        new_state = (max(0, min(n-1, state[0] + dr)), max(0, min(n-1, state[1] + dc)))\n",
    "        transitions.append((new_state, 0.2 / len(slip_actions), rewards.get(new_state, -1)))\n",
    "\n",
    "    return transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "RayKqgFZ0z0t"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Q-Value Iteration\n",
    "def q_value_iteration(gamma=0.9, theta=1e-6):\n",
    "    Q = {(s, a): 0 for s in states for a in actions}\n",
    "    while True:\n",
    "        delta = 0\n",
    "        new_Q = {}\n",
    "        for s in states:\n",
    "            for a in actions:\n",
    "                q_old = Q[(s, a)]\n",
    "                q_new = 0\n",
    "                for (s_prime, prob, r) in get_transitions(s, a):\n",
    "                    if s_prime in rewards:\n",
    "                        q_new += prob * r\n",
    "                    else:\n",
    "                        q_new += prob * (r + gamma * max(Q[(s_prime, a_prime)] for a_prime in actions))\n",
    "                new_Q[(s, a)] = q_new\n",
    "                delta = max(delta, abs(q_old - new_Q[(s, a)]))\n",
    "        Q = new_Q\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return Q\n",
    "\n",
    "# Run Q-Value Iteration\n",
    "Q = q_value_iteration()\n",
    "\n",
    "# Extract optimal policy\n",
    "policy = {}\n",
    "for s in states:\n",
    "    if s in rewards:\n",
    "        policy[s] = None  # Terminal state\n",
    "    else:\n",
    "        policy[s] = max(actions, key=lambda a: Q[(s, a)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rcBDlrPnQN8d",
    "outputId": "2a21d075-08a3-46fb-c07e-92614324ba10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Policy (Q-Value Iteration):\n",
      "    R     D   -10     D     D \n",
      "  -10     D   -10     D     D \n",
      "    D     R     D     L     L \n",
      "    D   -10     D   -10   -10 \n",
      "    R     R     R     R    10 \n",
      "--------------------------------------------------\n",
      "Optimal Policy (Learned via Q-Learning) and Q-values:\n",
      "   -6.3    -5.5     -10    -5.2    -5.4 \n",
      "    -10    -4.4     -10    -4.1    -4.8 \n",
      "   -2.9    -2.6    -0.8    -2.6    -4.0 \n",
      "   -1.3     -10     1.6     -10     -10 \n",
      "    0.8     2.4     5.3     8.0      10 \n"
     ]
    }
   ],
   "source": [
    "# Print policy\n",
    "print(\"Optimal Policy (Q-Value Iteration):\")\n",
    "for r in range(n):\n",
    "    for c in range(n):\n",
    "        s = (r, c)\n",
    "        if s in rewards:\n",
    "            print(f\"{rewards[s]:>5}\", end=\" \")\n",
    "        else:\n",
    "            print(f\"{policy[s]:>5}\", end=\" \")\n",
    "    print()\n",
    "print(\"-\" *50)\n",
    "print(\"Optimal Policy (Learned via Q-Learning) and Q-values:\")\n",
    "for r in range(n):\n",
    "    for c in range(n):\n",
    "        s = (r, c)\n",
    "        if s in rewards:  # Terminal state (no action)\n",
    "            print(f\"{rewards[s]:>7}\", end=\" \")\n",
    "        else:\n",
    "            best_action = policy[s]\n",
    "            best_q = Q[(s, best_action)]\n",
    "            print(f\"{best_q:>7.1f}\", end=\" \")\n",
    "    print()  # Newline after each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oj46aUfCQjVS"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
