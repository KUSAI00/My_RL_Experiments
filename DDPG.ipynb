{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting DDPG training on Pendulum-v1 for 101 episodes...\n",
      "Episode: 1/101, Training Reward: -1184.02\n",
      "Episode: 50/101, Training Reward: -127.64, Avg. Eval Reward: -128.97\n",
      "--- New best evaluation reward: -128.97. Agent weights saved. ---\n",
      "Episode: 51/101, Training Reward: -126.82\n",
      "Episode: 100/101, Training Reward: -125.64, Avg. Eval Reward: -125.62\n",
      "--- New best evaluation reward: -125.62. Agent weights saved. ---\n",
      "Episode: 101/101, Training Reward: -125.25\n",
      "\n",
      "Training complete. Generating video of best performance...\n",
      "Agent weights loaded from best_ddpg_agent.pth\n",
      "Video of best performance (reward: -125.62) saved to 'videos/best_ddpg_pendulum-0.mp4'\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "import copy\n",
    "import os\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "SEED = 0\n",
    "torch.manual_seed(SEED)\n",
    "np_random_seed = np.random.RandomState(SEED) # Use RandomState for consistent numpy randomness\n",
    "random.seed(SEED)\n",
    "\n",
    "# Define the device for training (GPU if available, else CPU)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 1. Network Definitions (Actor and Critic)\n",
    "class Actor(nn.Module):\n",
    "    \"\"\"Actor network to map states to actions.\"\"\"\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        super(Actor, self).__init__()\n",
    "        self.max_action = max_action\n",
    "        \n",
    "        # Define the layers of the actor network\n",
    "        self.l1 = nn.Linear(state_dim, 256)\n",
    "        self.l2 = nn.Linear(256, 256)\n",
    "        self.l3 = nn.Linear(256, action_dim)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Forward pass through the actor network.\"\"\"\n",
    "        x = F.relu(self.l1(state)) # First layer with ReLU activation\n",
    "        x = F.relu(self.l2(x)) # Second layer with ReLU activation\n",
    "        # The output is scaled by max_action and passed through tanh to constrain actions\n",
    "        return self.max_action * torch.tanh(self.l3(x))\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    \"\"\"Critic network to estimate Q-values for a given state-action pair.\"\"\"\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        \n",
    "        # Define the layers of the critic network\n",
    "        # Input includes both state and action dimensions\n",
    "        self.l1 = nn.Linear(state_dim + action_dim, 256)\n",
    "        self.l2 = nn.Linear(256, 256)\n",
    "        self.l3 = nn.Linear(256, 1) # Output is a single Q-value\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        \"\"\"Forward pass through the critic network.\"\"\"\n",
    "        # Concatenate state and action tensors\n",
    "        state_action = torch.cat([state, action], 1)\n",
    "        \n",
    "        x = F.relu(self.l1(state_action)) # First layer with ReLU activation\n",
    "        x = F.relu(self.l2(x)) # Second layer with ReLU activation\n",
    "        return self.l3(x) # Output the Q-value\n",
    "\n",
    "# 2. Replay Buffer\n",
    "class ReplayBuffer:\n",
    "    \"\"\"A buffer to store and sample past experiences.\"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity) # Use deque for efficient appending/popping\n",
    "\n",
    "    def push(self, transition):\n",
    "        \"\"\"Adds a new transition to the buffer.\"\"\"\n",
    "        self.buffer.append(transition)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Samples a random batch of transitions from the buffer.\"\"\"\n",
    "        # If the buffer is smaller than batch_size, sample all available transitions\n",
    "        batch_size = min(batch_size, len(self.buffer))\n",
    "        # Ensure we don't try to sample if the buffer is empty\n",
    "        if batch_size == 0:\n",
    "            return []\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the current size of the buffer.\"\"\"\n",
    "        return len(self.buffer)\n",
    "\n",
    "# 3. Ornstein-Uhlenbeck Noise for Exploration\n",
    "class OUNoise:\n",
    "    \"\"\"\n",
    "    Ornstein-Uhlenbeck process.\n",
    "    Used for exploration in continuous action spaces.\n",
    "    It generates temporally correlated noise, helping the agent explore smoothly.\n",
    "    \"\"\"\n",
    "    def __init__(self, action_dim, mu=0.0, theta=0.15, sigma=0.3): # Increased sigma\n",
    "        self.action_dim = action_dim\n",
    "        self.mu = mu # Mean of the noise\n",
    "        self.theta = theta # Rate of mean reversion\n",
    "        self.sigma = sigma # Volatility of the noise\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.deepcopy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "        # Calculate dx based on the OU process formula\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np_random_seed.randn(self.action_dim)\n",
    "        self.state = x + dx\n",
    "        return self.state\n",
    "\n",
    "# 4. DDPG Agent\n",
    "class DDPG:\n",
    "    \"\"\"The DDPG agent that manages the actor-critic networks and the learning process.\"\"\"\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        self.device = DEVICE\n",
    "\n",
    "        # Main networks (Actor and Critic)\n",
    "        self.actor = Actor(state_dim, action_dim, max_action).to(self.device)\n",
    "        self.critic = Critic(state_dim, action_dim).to(self.device)\n",
    "\n",
    "        # Target networks (Actor_target and Critic_target) for stability\n",
    "        self.actor_target = Actor(state_dim, action_dim, max_action).to(self.device)\n",
    "        self.critic_target = Critic(state_dim, action_dim).to(self.device)\n",
    "\n",
    "        # Initialize target networks with the same weights as main networks\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "\n",
    "        # Optimizers for both networks\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=1e-4)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=1e-3)\n",
    "\n",
    "        self.replay_buffer = ReplayBuffer(capacity=1_000_000) # Increased capacity\n",
    "        self.gamma = 0.99  # Discount factor for future rewards\n",
    "        self.tau = 0.005  # Soft update parameter for target networks\n",
    "\n",
    "        # Ornstein-Uhlenbeck noise for exploration\n",
    "        self.ou_noise = OUNoise(action_dim)\n",
    "\n",
    "\n",
    "    def select_action(self, state, add_noise=True):\n",
    "        \"\"\"\n",
    "        Selects a deterministic action from the actor network.\n",
    "        Adds OU noise for exploration during training if add_noise is True.\n",
    "        \"\"\"\n",
    "        # Convert state to a PyTorch tensor and move to the correct device\n",
    "        state = torch.FloatTensor(state.reshape(1, -1)).to(self.device)\n",
    "        \n",
    "        # Get the action from the actor network\n",
    "        self.actor.eval() # Set actor to evaluation mode\n",
    "        with torch.no_grad(): # Disable gradient calculations\n",
    "            action = self.actor(state).cpu().data.numpy().flatten()\n",
    "        self.actor.train() # Set actor back to training mode\n",
    "        \n",
    "        if add_noise:\n",
    "            # Add OU noise to the action\n",
    "            noise_sample = self.ou_noise.sample()\n",
    "            action = action + noise_sample\n",
    "            # Clip the action to ensure it stays within the valid range\n",
    "            action = np.clip(action, -self.actor.max_action, self.actor.max_action)\n",
    "            \n",
    "        return action\n",
    "\n",
    "    def train(self, batch_size):\n",
    "        \"\"\"Performs a single training step to update networks.\"\"\"\n",
    "        # Ensure enough experiences are in the buffer for a batch\n",
    "        if len(self.replay_buffer) < batch_size:\n",
    "            return\n",
    "\n",
    "        transitions = self.replay_buffer.sample(batch_size)\n",
    "        \n",
    "        # Handle cases where buffer might be sampled empty due to min(batch_size, len(buffer))\n",
    "        if not transitions:\n",
    "            return\n",
    "\n",
    "        # Unzip the batch of transitions\n",
    "        state_batch, action_batch, reward_batch, next_state_batch, done_batch = zip(*transitions)\n",
    "\n",
    "        # Convert numpy arrays to PyTorch tensors and move to the correct device\n",
    "        state = torch.FloatTensor(np.array(state_batch)).to(self.device)\n",
    "        action = torch.FloatTensor(np.array(action_batch)).to(self.device)\n",
    "        reward = torch.FloatTensor(np.array(reward_batch)).unsqueeze(1).to(self.device)\n",
    "        next_state = torch.FloatTensor(np.array(next_state_batch)).to(self.device)\n",
    "        # Convert done flag to float and move to device\n",
    "        done = torch.FloatTensor(np.array(done_batch).astype(float)).unsqueeze(1).to(self.device)\n",
    "\n",
    "        # --- Update Critic Network ---\n",
    "        with torch.no_grad(): # No gradients needed for target computations\n",
    "            # Get next actions from the actor target network\n",
    "            next_action = self.actor_target(next_state)\n",
    "            # Calculate target Q-values using critic target network\n",
    "            target_Q = self.critic_target(next_state, next_action)\n",
    "            # Bellman equation for target Q-value\n",
    "            target_Q = reward + ( (1 - done) * self.gamma * target_Q )\n",
    "\n",
    "        # Get current Q-values from the main critic network\n",
    "        current_Q = self.critic(state, action)\n",
    "        # Calculate critic loss (Mean Squared Error between current and target Q-values)\n",
    "        critic_loss = F.mse_loss(current_Q, target_Q)\n",
    "\n",
    "        # Optimize critic network\n",
    "        self.critic_optimizer.zero_grad() # Clear previous gradients\n",
    "        critic_loss.backward() # Backpropagate loss\n",
    "        self.critic_optimizer.step() # Update critic weights\n",
    "\n",
    "        # --- Update Actor Network ---\n",
    "        # Calculate actor loss (maximize Q-value predicted by critic for actor's actions)\n",
    "        actor_loss = -self.critic(state, self.actor(state)).mean()\n",
    "        \n",
    "        # Optimize actor network\n",
    "        self.actor_optimizer.zero_grad() # Clear previous gradients\n",
    "        actor_loss.backward() # Backpropagate loss\n",
    "        self.actor_optimizer.step() # Update actor weights\n",
    "\n",
    "        # --- Soft Update Target Networks ---\n",
    "        # Update critic target network\n",
    "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "        # Update actor target network\n",
    "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "    def save_weights(self, filename=\"best_ddpg_agent.pth\"):\n",
    "        \"\"\"Saves the state dictionaries of the actor and critic networks.\"\"\"\n",
    "        torch.save({\n",
    "            'actor_state_dict': self.actor.state_dict(),\n",
    "            'critic_state_dict': self.critic.state_dict(),\n",
    "            'actor_target_state_dict': self.actor_target.state_dict(),\n",
    "            'critic_target_state_dict': self.critic_target.state_dict(),\n",
    "            'actor_optimizer_state_dict': self.actor_optimizer.state_dict(),\n",
    "            'critic_optimizer_state_dict': self.critic_optimizer.state_dict(),\n",
    "        }, filename)\n",
    "        # print(f\"Agent weights saved to {filename}\") # Suppress frequent save messages\n",
    "\n",
    "    def load_weights(self, filename=\"best_ddpg_agent.pth\"):\n",
    "        \"\"\"Loads the state dictionaries into the actor and critic networks.\"\"\"\n",
    "        if not os.path.exists(filename):\n",
    "            print(f\"Error: {filename} not found. Cannot load weights.\")\n",
    "            return\n",
    "\n",
    "        checkpoint = torch.load(filename, map_location=self.device)\n",
    "        self.actor.load_state_dict(checkpoint['actor_state_dict'])\n",
    "        self.critic.load_state_dict(checkpoint['critic_state_dict'])\n",
    "        self.actor_target.load_state_dict(checkpoint['actor_target_state_dict'])\n",
    "        self.critic_target.load_state_dict(checkpoint['critic_target_state_dict'])\n",
    "        self.actor_optimizer.load_state_dict(checkpoint['actor_optimizer_state_dict'])\n",
    "        self.critic_optimizer.load_state_dict(checkpoint['critic_optimizer_state_dict'])\n",
    "        print(f\"Agent weights loaded from {filename}\")\n",
    "\n",
    "\n",
    "def evaluate_agent(env, agent, num_eval_episodes=1, seed=SEED):\n",
    "    \"\"\"\n",
    "    Evaluates the agent's performance over a specified number of episodes\n",
    "    without exploration noise.\n",
    "    \"\"\"\n",
    "    total_rewards = []\n",
    "    for _ in range(num_eval_episodes):\n",
    "        state, _ = env.reset(seed=seed)\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = agent.select_action(state, add_noise=False) # No noise for evaluation\n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            episode_reward += reward\n",
    "        total_rewards.append(episode_reward)\n",
    "    return np.mean(total_rewards)\n",
    "\n",
    "\n",
    "# 5. Training Loop\n",
    "def main():\n",
    "    env = gym.make(\"Pendulum-v1\")\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0]\n",
    "    max_action = float(env.action_space.high[0])\n",
    "\n",
    "    agent = DDPG(state_dim, action_dim, max_action)\n",
    "\n",
    "    num_episodes = 101 # Increased number of training episodes\n",
    "    batch_size = 128\n",
    "    eval_interval = 50 # Evaluate agent every 50 episodes\n",
    "\n",
    "    best_eval_reward = -np.inf # Track the best evaluation reward achieved\n",
    "    best_agent_path = \"best_ddpg_agent.pth\" # File to save best agent weights\n",
    "\n",
    "    print(f\"Starting DDPG training on Pendulum-v1 for {num_episodes} episodes...\")\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset(seed=SEED) # Reset environment for new episode\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        step = 0\n",
    "\n",
    "        # Reset OU noise for the new episode\n",
    "        agent.ou_noise.reset() \n",
    "\n",
    "        while not done:\n",
    "            # Select action with OU noise for exploration during training\n",
    "            action = agent.select_action(state, add_noise=True)\n",
    "\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            agent.replay_buffer.push((state, action, reward, next_state, done))\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            step += 1\n",
    "\n",
    "            # Train the agent (only if buffer has enough samples)\n",
    "            agent.train(batch_size)\n",
    "\n",
    "        # Evaluate agent periodically\n",
    "        if (episode + 1) % eval_interval == 0:\n",
    "            avg_eval_reward = evaluate_agent(env, agent, num_eval_episodes=5, seed=SEED)\n",
    "            print(f\"Episode: {episode + 1}/{num_episodes}, Training Reward: {episode_reward:.2f}, \"\n",
    "                  f\"Avg. Eval Reward: {avg_eval_reward:.2f}\")\n",
    "\n",
    "            if avg_eval_reward > best_eval_reward:\n",
    "                best_eval_reward = avg_eval_reward\n",
    "                agent.save_weights(best_agent_path) # Save weights if this is the best evaluation so far\n",
    "                print(f\"--- New best evaluation reward: {best_eval_reward:.2f}. Agent weights saved. ---\")\n",
    "        elif episode % 50 == 0: # Print training reward for other intervals\n",
    "            print(f\"Episode: {episode + 1}/{num_episodes}, Training Reward: {episode_reward:.2f}\")\n",
    "\n",
    "\n",
    "    env.close()\n",
    "    print(\"\\nTraining complete. Generating video of best performance...\")\n",
    "\n",
    "    # --- Generate video of the best-performing agent ---\n",
    "    # Create a directory for videos if it doesn't exist\n",
    "    video_dir = \"videos\"\n",
    "    os.makedirs(video_dir, exist_ok=True)\n",
    "\n",
    "    # Create a new environment wrapped with RecordVideo\n",
    "    # render_mode=\"rgb_array\" is required for recording\n",
    "    eval_env_video = gym.make(\"Pendulum-v1\", render_mode=\"rgb_array\")\n",
    "    eval_env_video = gym.wrappers.RecordVideo(\n",
    "        eval_env_video, \n",
    "        video_folder=video_dir, \n",
    "        name_prefix=\"best_ddpg_pendulum\",\n",
    "        episode_trigger=lambda x: True # Record every episode\n",
    "    )\n",
    "    \n",
    "    # Create a new agent for evaluation\n",
    "    eval_agent = DDPG(state_dim, action_dim, max_action)\n",
    "    eval_agent.load_weights(best_agent_path) # Load the best weights\n",
    "\n",
    "    # Run one episode with the best agent and record it\n",
    "    state, _ = eval_env_video.reset(seed=SEED)\n",
    "    done = False\n",
    "    total_eval_reward_video = 0\n",
    "\n",
    "    while not done:\n",
    "        # Select action without noise for deterministic evaluation\n",
    "        action = eval_agent.select_action(state, add_noise=False)\n",
    "        state, reward, terminated, truncated, _ = eval_env_video.step(action)\n",
    "        done = terminated or truncated\n",
    "        total_eval_reward_video += reward\n",
    "    \n",
    "    eval_env_video.close()\n",
    "    print(f\"Video of best performance (reward: {total_eval_reward_video:.2f}) saved to '{video_dir}/best_ddpg_pendulum-0.mp4'\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Initialize numpy random state globally for OUNoise\n",
    "    np_random_seed = np.random.RandomState(SEED)\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <video controls autoplay width=\"700\">\n",
       "        <source src=\"videos/best_ddpg_pendulum-episode-0.mp4\" type=\"video/mp4\">\n",
       "        Your browser does not support the video tag.\n",
       "    </video>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML, display\n",
    "import os\n",
    "\n",
    "video_path = \"videos/best_ddpg_pendulum-episode-0.mp4\" \n",
    "\n",
    "if os.path.exists(video_path):\n",
    "    display(HTML(f\"\"\"\n",
    "    <video controls autoplay width=\"700\">\n",
    "        <source src=\"{video_path}\" type=\"video/mp4\">\n",
    "        Your browser does not support the video tag.\n",
    "    </video>\n",
    "    \"\"\"))\n",
    "else:\n",
    "    print(f\"Error: Video file not found at {os.path.abspath(video_path)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
