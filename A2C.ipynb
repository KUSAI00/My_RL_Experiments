{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Op41lSMz59mm",
    "outputId": "e98f9c01-1ad8-49de-a4e3-dadde35bf028"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: swig in /usr/local/lib/python3.11/dist-packages (4.3.1)\n",
      "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.11/dist-packages (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.0.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.14.1)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
      "Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n",
      "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.6.1)\n",
      "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.3.1)\n",
      "Building wheels for collected packages: box2d-py\n",
      "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp311-cp311-linux_x86_64.whl size=2379375 sha256=d2c4c8ff8d3318fb715c191ad8cca583cd1886e09d01edce671416964b167b35\n",
      "  Stored in directory: /root/.cache/pip/wheels/ab/f1/0c/d56f4a2bdd12bae0a0693ec33f2f0daadb5eb9753c78fa5308\n",
      "Successfully built box2d-py\n",
      "Installing collected packages: box2d-py\n",
      "Successfully installed box2d-py-2.3.5\n"
     ]
    }
   ],
   "source": [
    "!pip install swig\n",
    "!pip install \"gymnasium[box2d]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Up8Damz_5sY6",
    "outputId": "5b3ac729-0b78-4803-de8c-775a124075e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training on LunarLander-v3...\n",
      "State space dimension: 8\n",
      "Action space dimension: 4\n",
      "Episode 50, Reward: -203.3598829224493\n",
      "Episode 100, Reward: -483.3620857665338\n",
      "Episode 150, Reward: -199.40160871888105\n",
      "Episode 200, Reward: -180.3695319392446\n",
      "Episode 250, Reward: -393.51382411637377\n",
      "Episode 300, Reward: -316.79128414559534\n",
      "Episode 350, Reward: -10.353742055773637\n",
      "Episode 400, Reward: -213.50247951431436\n",
      "Episode 450, Reward: -299.47945337662617\n",
      "Episode 500, Reward: -61.6104767649923\n",
      "Episode 550, Reward: -211.9034591873595\n",
      "Episode 600, Reward: -6.9788440833922465\n",
      "Episode 650, Reward: 39.67398080978191\n",
      "Episode 700, Reward: -271.7822727420509\n",
      "Episode 750, Reward: -15.593473725284554\n",
      "Episode 800, Reward: -41.452652517768456\n",
      "Episode 850, Reward: -55.644293561977435\n",
      "Episode 900, Reward: -63.85697810361698\n",
      "Episode 950, Reward: -166.29564709615906\n",
      "Episode 1000, Reward: -17.530074554587326\n",
      "Episode 1050, Reward: -39.65788343165494\n",
      "Episode 1100, Reward: 38.906078556604186\n",
      "Episode 1150, Reward: -210.7900128882148\n",
      "Episode 1200, Reward: 33.7973490648136\n",
      "Episode 1250, Reward: 22.292249510040833\n",
      "Episode 1300, Reward: 45.256699691621385\n",
      "Episode 1350, Reward: 3.515297813458285\n",
      "Episode 1400, Reward: 78.55753590959785\n",
      "Episode 1450, Reward: 63.49662500735825\n",
      "Episode 1500, Reward: 71.64545209819471\n",
      "Episode 1550, Reward: 105.12732570593245\n",
      "Episode 1600, Reward: 4.105790915269267\n",
      "Episode 1650, Reward: -18.555238384764664\n",
      "Episode 1700, Reward: 102.96431340748283\n",
      "Episode 1750, Reward: 35.26058699871716\n",
      "Episode 1800, Reward: 147.94944154537544\n",
      "Episode 1850, Reward: 136.71448324207694\n",
      "Episode 1900, Reward: 24.181531689625928\n",
      "Episode 1950, Reward: 86.92237021880035\n",
      "Episode 2000, Reward: 115.39879226808883\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "# --- Actor-Critic Network ---\n",
    "# This class defines the neural network architecture for both the actor and the critic.\n",
    "# For the more complex LunarLander environment, we'll use a slightly deeper network\n",
    "# with two hidden layers to better capture the state features.\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    \"\"\"\n",
    "    A neural network for the Actor-Critic agent, adapted for LunarLander.\n",
    "\n",
    "    This network takes a state as input and outputs two things:\n",
    "    1. A probability distribution over actions (the policy), from the actor.\n",
    "    2. An estimate of the value of the state, from the critic.\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=256):\n",
    "        super(ActorCritic, self).__init__()\n",
    "\n",
    "        # Shared layers are now deeper for a more complex environment\n",
    "        self.shared_layers = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Actor-specific layer\n",
    "        # This layer outputs logits for each action. A softmax will be applied\n",
    "        # later to get a probability distribution.\n",
    "        self.actor_head = nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "        # Critic-specific layer\n",
    "        # This layer outputs a single value, which is the estimated value of the state.\n",
    "        self.critic_head = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "        Performs a forward pass through the network.\n",
    "\n",
    "        Args:\n",
    "            state (torch.Tensor): The current state of the environment.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing:\n",
    "                - action_probs (torch.distributions.Categorical): A distribution over actions.\n",
    "                - state_value (torch.Tensor): The estimated value of the input state.\n",
    "        \"\"\"\n",
    "        # Pass the state through the shared layers\n",
    "        shared_features = self.shared_layers(state)\n",
    "\n",
    "        # Get the action logits from the actor head\n",
    "        action_logits = self.actor_head(shared_features)\n",
    "        # Create a categorical distribution from the logits\n",
    "        action_probs = Categorical(F.softmax(action_logits, dim=-1))\n",
    "\n",
    "        # Get the state value from the critic head\n",
    "        state_value = self.critic_head(shared_features)\n",
    "\n",
    "        return action_probs, state_value\n",
    "\n",
    "\n",
    "# --- A2C Agent ---\n",
    "# This class brings everything together. It contains the actor-critic network,\n",
    "# the optimizer, and the logic for training the agent. The logic remains the same\n",
    "# as it is a general implementation of A2C.\n",
    "\n",
    "class A2CAgent:\n",
    "    \"\"\"\n",
    "    The Advantage Actor-Critic (A2C) agent.\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim, action_dim, learning_rate=0.0005, gamma=0.99):\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # Initialize the Actor-Critic network\n",
    "        self.ac_network = ActorCritic(state_dim, action_dim)\n",
    "        # Initialize the optimizer\n",
    "        self.optimizer = optim.Adam(self.ac_network.parameters(), lr=learning_rate)\n",
    "\n",
    "        # These lists will store the experiences of one episode\n",
    "        self.log_probs = []\n",
    "        self.values = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "\n",
    "    def select_action(self, state):\n",
    "        \"\"\"\n",
    "        Selects an action based on the current policy.\n",
    "\n",
    "        Args:\n",
    "            state (np.ndarray): The current state of the environment.\n",
    "\n",
    "        Returns:\n",
    "            int: The action to take.\n",
    "        \"\"\"\n",
    "        # Convert the state to a PyTorch tensor\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        # Get the action probabilities and state value from the network\n",
    "        action_probs, state_value = self.ac_network(state)\n",
    "\n",
    "        # Sample an action from the distribution\n",
    "        action = action_probs.sample()\n",
    "\n",
    "        # Store the log probability of the action and the state value\n",
    "        self.log_probs.append(action_probs.log_prob(action))\n",
    "        self.values.append(state_value)\n",
    "\n",
    "        return action.item()\n",
    "\n",
    "    def update(self):\n",
    "        \"\"\"\n",
    "        Updates the actor and critic networks.\n",
    "        \"\"\"\n",
    "        # Convert lists to tensors\n",
    "        log_probs = torch.cat(self.log_probs)\n",
    "        values = torch.cat(self.values).squeeze()\n",
    "        rewards = torch.tensor(self.rewards)\n",
    "        dones = torch.tensor(self.dones, dtype=torch.float32)\n",
    "\n",
    "        # Calculate returns (discounted rewards)\n",
    "        returns = []\n",
    "        discounted_reward = 0\n",
    "        for reward, done in zip(reversed(self.rewards), reversed(self.dones)):\n",
    "            if done:\n",
    "                discounted_reward = 0\n",
    "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
    "            returns.insert(0, discounted_reward)\n",
    "\n",
    "        returns = torch.tensor(returns).float()  # <-- FIX: Convert to float32 here\n",
    "        # Normalize returns for more stable training\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-9)\n",
    "\n",
    "        # Calculate advantage\n",
    "        # Advantage = Returns - Values\n",
    "        advantage = returns - values\n",
    "\n",
    "        # Calculate actor loss (policy gradient loss)\n",
    "        actor_loss = -(log_probs * advantage.detach()).mean()\n",
    "\n",
    "        # Calculate critic loss (mean squared error between returns and values)\n",
    "        critic_loss = F.mse_loss(returns, values)\n",
    "\n",
    "        # Total loss\n",
    "        loss = actor_loss + 0.5 * critic_loss\n",
    "\n",
    "        # Perform backpropagation\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Clear the memory for the next episode\n",
    "        self.log_probs = []\n",
    "        self.values = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "\n",
    "def main():\n",
    "    # Create the environment - Changed to LunarLander-v3\n",
    "    env = gym.make('LunarLander-v3')  # This is the only change needed.\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "\n",
    "    # Create the A2C agent with a potentially adjusted learning rate\n",
    "    agent = A2CAgent(state_dim, action_dim, learning_rate=5e-4)\n",
    "\n",
    "    # Training parameters adjusted for LunarLander\n",
    "    num_episodes = 2000\n",
    "    max_steps_per_episode = 1000\n",
    "\n",
    "    print(f\"Starting training on LunarLander-v3...\")\n",
    "    print(f\"State space dimension: {state_dim}\")\n",
    "    print(f\"Action space dimension: {action_dim}\")\n",
    "\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "\n",
    "        for step in range(max_steps_per_episode):\n",
    "            # Select an action\n",
    "            action = agent.select_action(state)\n",
    "            # Take the action in the environment\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # Store the experience\n",
    "            agent.rewards.append(reward)\n",
    "            agent.dones.append(done)\n",
    "\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # Update the agent at the end of the episode\n",
    "        agent.update()\n",
    "\n",
    "        if (episode + 1) % 50 == 0:\n",
    "            print(f\"Episode {episode + 1}, Reward: {episode_reward}\")\n",
    "\n",
    "    env.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ozXERtWQOjHF"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
